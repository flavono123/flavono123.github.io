<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>kubernetes on flavono123</title><link>https://flavono123.github.io/tags/kubernetes/</link><description>Recent content in kubernetes on flavono123</description><generator>Hugo -- gohugo.io</generator><language>ko-kr</language><lastBuildDate>Tue, 21 Jun 2022 18:46:55 +0900</lastBuildDate><atom:link href="https://flavono123.github.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>Kubectl Patch</title><link>https://flavono123.github.io/posts/kubectl-json-patch-merge/</link><pubDate>Tue, 21 Jun 2022 18:46:55 +0900</pubDate><guid>https://flavono123.github.io/posts/kubectl-json-patch-merge/</guid><description>JSON Patch kubectl patch 서브명령은 바꿀 JSON을 인자로 실행 중인 객체의 정의를 바꾼다. edit 보다 편하기도, 또 누군가에게 설명하기 쉬운 경우도 많다.
하지만 인라인으로 쓰인 patch 인자가 복잡해 보일 때도 있다. 이때 보다 나은, JSON Patch를 사용할 수 있다:
$ k run patch-test --image busybox -- sleep 1d pod/patch-test created # k label po patch-test test=true 와 같다 $ k patch po patch-test --type json \ -p &amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;add&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/metadata/labels/test&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;true&amp;#34;}]&amp;#39; pod/patch-test patched $ k get po -l test=true NAME READY STATUS RESTARTS AGE patch-test 1/1 Running 0 108s Patch할 JSON에 여러 변경사항(연산)을 인자로 쓸 수 있지만, 하날 쓰더라도 무조건 배열에 담아 보내야 하는 것을 주의하자.</description></item><item><title>Yq 중첩 키 경로 파악하기(긴 Helm 차트 values 파악하기)</title><link>https://flavono123.github.io/posts/yq-parse-long-nested-helm-values/</link><pubDate>Mon, 20 Jun 2022 19:40:31 +0900</pubDate><guid>https://flavono123.github.io/posts/yq-parse-long-nested-helm-values/</guid><description>Yq의 globbing과 내장 연산자 몇가지를 사용하면 중첩된 객체에서 특정 키 경로를 알아낼 수 있다:
❯ cat test.yaml a: b: target: HERE ❯ yq e &amp;#39; .. | select(has(&amp;#34;target&amp;#34;)) | path&amp;#39; test.yaml - a - b 실전으론, 아주 긴 쿠버네티스 매니페스트나 Helm values를 파악할 때 사용할 수 있다. 하지만 내가 쓰는 방법이 완전한 해결책이라는 생각이 들진 않는데&amp;hellip; 일단 지금 쓰는 방식을 이야기 해본다.
한 예로 kube-prometheus-stack의 values는 아주 길다:
$ helm show values prometheus-community/kube-prometheus-stack --version 36.</description></item><item><title>HPA</title><link>https://flavono123.github.io/posts/hpa/</link><pubDate>Mon, 20 Jun 2022 10:27:40 +0900</pubDate><guid>https://flavono123.github.io/posts/hpa/</guid><description>쿠버네티스에서 오토스케일은 크게 세가지가 있다. 클러스터 오토스케일러, 수직적 파드 오토스케일러(Vertical Pod Autoscaler; VPA) 그리고 수평적 파드 오토스케일러(Horizontal Pod Autoscaler)이다. 클러스터는 노드 수준 그리고 뒤의 두가지는 파드 수준의 오토스케일이다.
클러스터 오토스케일러는 노드 개수를 스케일 아웃/인 한다. VPA는 파드에 할당되는 컴퓨팅 자원(CPU, 메모리의) 스케일 업/다운을 그리고 HPA는 파드 개수(replicas)의 스케일 아웃/인 한다(HPA 관련한 쿠버네티스 문서엔 전부 스케일 업/다운이라는 표현을 쓰지만 통상적인 의미의 스케일 아웃/인이라고 해석했다).
클러스터 오토스케일러와 VPA는 쿠버네티스 API로 제공되지 않고 저장소도 분리되어 있다.</description></item><item><title>Sysbench로 MySQL Operator 벤치마크</title><link>https://flavono123.github.io/posts/sysbench-mysql-operator/</link><pubDate>Tue, 07 Jun 2022 20:17:31 +0900</pubDate><guid>https://flavono123.github.io/posts/sysbench-mysql-operator/</guid><description>쿠버네티스 데이터베이스 오퍼레이터 스터디 2주차 도전과제로 MySQL Operator 설치 후 Sysbench로 벤치마크하는 과제를 진행했다. 여기서 벤치마크는, Kubestr 실습과 비슷하게, 하드웨어 또는 쿠버네티스 클러스터 구성이나 오퍼레이터와 같은 실제 인프라에 대한 벤치마크가 아닌 벤치마크 연습 정도이다.
MySQL 서버 인스턴스를 최소 구성인 세대가 노드 하나씩에 분산하기 위해 워커 노드 하나를 추가했다. 먼저 MySQL Operator를 헬름 차트로 설치해, 문서의 설명과 설치된 자원을 확인해본다. 그리고 sysbench를 실행해 결과를 확인한다.
MySQL Operator 다음 명령으로 MySQL Operator 헬름 차트를 설치한다:</description></item><item><title>Kubestr</title><link>https://flavono123.github.io/posts/kubestr-and-monitoring-tools/</link><pubDate>Mon, 30 May 2022 23:24:25 +0900</pubDate><guid>https://flavono123.github.io/posts/kubestr-and-monitoring-tools/</guid><description>kubestr라는 쿠버네티스 저장소 IO 벤치마크 도구로 저장소 성능을 측정해본다. 지난 글에서 만든 local-path-provisioner와 nfs-subdir-external-provisioner를 새로 설치해 둘을 비교한다. 로컬 VM에서 하는 것이기 때문에 실제 성능 측정보단 벤치마크 자체를 연습해본다. 또 벤치마킹하는 동안 컴퓨팅 자원 지표를 측정할 수 있는 모니터 도구 sar를 사용해본다.
NFS nfs-subdir-external-provisioner를 설치하기 위해 노드에 NFS를 마운트한다. 별도 NFS 노드를 준비하지 않고 controlplane에 마운트한다:
--- # variables nfs_mount_path: /nfs-storage nfs_network_cidr: 192.168.1.0/24 --- # tasks - name: Turn up NFS in controlplane become: yes block: - name: Install nfs-kernel-server apt: state: present update_cache: yes name: - nfs-kernel-server - name: Create NFS mount path file: state: directory owner: nobody group: nogroup mode: &amp;#34;0777&amp;#34; path: &amp;#34;{{ nfs_mount_path }}&amp;#34; - name: Add exports table to /etc/exports blockinfile: path: /etc/exports block: | {{ nfs_mount_path }} {{ nfs_network_cidr }}(rw,sync,no_subtree_check) - name: Export /etc/exports command: exportfs -a - name: Restart nfs-kernel-server service command: systemctl restart nfs-kernel-server when: - &amp;#34;&amp;#39;controlplane&amp;#39; in group_names&amp;#34; - name: Install nfs-client become: yes apt: state: present update_cache: yes name: - nfs-common controlplane엔:</description></item><item><title>Statefulset(vs. Deployment)</title><link>https://flavono123.github.io/posts/statefulset-vs-deployment/</link><pubDate>Thu, 26 May 2022 12:54:04 +0900</pubDate><guid>https://flavono123.github.io/posts/statefulset-vs-deployment/</guid><description>쿠버네티스 문서의 스테이트풀셋 기본 실습을 따라해본다. 추가로, 기본적으로 스테이트리스의 파드셋을 관리하는, 디플로이먼트와 비교해본다. 하지만 스테이트풀셋과 디플로이먼트가 완전한 대척점에 있다고 보긴 어려워, 설명은 스테이트풀셋 위주로 할 것이다({스테이트풀셋의 기능} - {디플로이먼트의 기능}을 설명하게 된다).
스테이트풀셋은 공부하게 된 계기는 DOIK(Database Operator in Kubernetes)라는 스터디에 참여했기 때문이다. 스터디는 우연한 계기로 알게 됐는데, 스테이트풀셋이 실제 제품에서 사용 가능할까? 라는 의문이 있었기 때문에 참여했다. 스터디 이름에서 알 수 있듯, 실제론 오퍼레이터라는 설계 패턴으로 구현하는 것 같다. 하지만 DB를 컨테이너로 운영하는 기본은 스테이트풀셋이라 1주차엔 이것에 대한 스터디를 했다.</description></item><item><title>kubeadm으로 쿠버네티스 클러스터 설치와 문제 해결</title><link>https://flavono123.github.io/posts/kubeadm-init-network-troubleshootings/</link><pubDate>Tue, 24 May 2022 17:24:19 +0900</pubDate><guid>https://flavono123.github.io/posts/kubeadm-init-network-troubleshootings/</guid><description>쿠버네티스 클러스터를 turn up하고 그 과정에서 생긴 문제점과 해결 과정을 정리한다. 코드는 이 레포에 있고 글을 쓰는 시점에 마지막 커밋은 0188346이다.
노드엔 Vagrant, 클러스터 구성 도구는 kubeadm 그리고 CNI는 Calico를 사용했다. Vagrant 컴퓨팅 리소스를 보면 알 수 있듯, 로컬에서 연습용으로 사용할 클러스터이다.
provisioning 디렉토리 아래의 Ansible 코드 위주로 turnup 과정을 먼저 설명한다. 그리고 대부분 네트워크 문제였던 문제 해결 과정을 후술한다.
Prerequisuite 클러스터는 총 3개 노드 모두 ubuntu 20.04이고, 각각 controlplane 1개와 worker2 개이다.</description></item><item><title>K8s CRI와 containerd 설치</title><link>https://flavono123.github.io/posts/containerd-as-kubernetes-cri/</link><pubDate>Sun, 08 May 2022 16:45:29 +0900</pubDate><guid>https://flavono123.github.io/posts/containerd-as-kubernetes-cri/</guid><description>kubeadm으로 쿠버네티스 클러스터를 구성하던 중 컨테이너 런타임 설치 부분에서 혼란이 있었다. 전부터 containerd, 런타임, 쿠버네티스의 도커 지원 중단 같은 이야기를 들었지만 명확히 알진 않았다.
따라서 컨테이너 런타임이 무엇인지, 또 구성하게 될 쿠버네티스 클러스터에선 containerd가 어떤 역할을 할지 정리한다.
도커 역사와 구조 변화 이 내용은 Docker Deep Dive의 파트1 2장 Docker와 파트2 5장 The Docker Engine을 참고했다. 그리고 책에서 말하듯, 도커의 구조는 도커와 OCI의 역사 관련한 설명을 빼고 설명하기가 어렵다. 특히 역사, 과거의 사건, 관련해선 저자의 말을 빌려 간단하게 압축하여 설명한다.</description></item><item><title>NetworkPolicy 설정 시 주의점</title><link>https://flavono123.github.io/posts/notes-for-netpol/</link><pubDate>Fri, 04 Mar 2022 20:57:52 +0900</pubDate><guid>https://flavono123.github.io/posts/notes-for-netpol/</guid><description>CKAD killer.sh 시뮬레이터를 풀다가, 배점이 가장 높은 문제에서 계속 틀렸다. 이번에도 네트워크 정책 문제였다. 이번에도 몇가지 실험해서 알게 된 점을 정리한다.
문제에서 egress 설정을 잘한거 같은데 안됐다. 예를 들면 1234로 서비스 노출한 특정 파드(nginx)에 특정 파드만(san) 접근할 수 있게 하는 것이다:
❯ k -n netpol-test run nginx --image=nginx --port 80 pod/nginx created ❯ k -n netpol-test expose po nginx --port=1234 --target-port=80 service/nginx exposed apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: san-to-nginx namespace: netpol-test spec: podSelector: matchLabels: run: san policyTypes: - Egress egress: - to: - podSelector: matchLabels: run: nginx ports: - protocol: TCP port: 1234 ❯ k -n netpol-test run san --rm -i --image=busybox --restart=Never -- nc -zv -w 3 nginx 1234 If you don&amp;#39;t see a command prompt, try pressing enter.</description></item><item><title>nc(Netcat)으로 NetworkPolicy 디버깅</title><link>https://flavono123.github.io/posts/debug-netpol-with-nc/</link><pubDate>Thu, 03 Mar 2022 20:30:09 +0900</pubDate><guid>https://flavono123.github.io/posts/debug-netpol-with-nc/</guid><description>CKAD 문제를 풀던 중 네트워크 정책을 설정하고 검증해야하는 문제가 있었는데 telnet 명령이 들질 않았다:
root@controlplane:~# k exec &amp;lt;from-pod&amp;gt; -- telnet &amp;lt;svc&amp;gt; 80 OCI runtime exec failed: exec failed: container_linux.go:367: starting container process caused: exec: &amp;#34;telnet&amp;#34;: executable file not found in $PATH: unknown command terminated with exit code 126 답지에선 nc(Netcat)을 사용해서 검증했다. nc라는 명령을 처음 알게 됐다. 문제를 재구성 해봤고 그 과정에서 알게 된 사실을 공유한다.
nc 앞서 말한 nc는 telnet과 비슷하게 호스트의 포트가 열려 있는지 간단하게 확인 가능하다.</description></item><item><title>CKA with Practice Tests 정리: Scheduler</title><link>https://flavono123.github.io/posts/cka-3-scheduler/</link><pubDate>Sun, 23 Jan 2022 13:21:29 +0900</pubDate><guid>https://flavono123.github.io/posts/cka-3-scheduler/</guid><description>Manage scheduling 스케줄러는 파드를 어떤 노드에 할당(bind)할지 판단한다. 지금까지 파드 생성 시 정의에 명시하지 않았지만, spec.nodeName에 할당할 파드를 명시할 수 있다. 이런 방법은 추천하지 않는것 같고, 스케줄러에게 맡기되 그걸 제어할 수 있는 방법을 이번 장에서 다룬다.
스케줄러는 core Binding API(target.kind: Node)를 이용해 특정 노드에 파드 할당을 요청한다.
스케줄러는 컨트롤플레인 노드에서, 뒤에서 설명할, 스태틱 파드로 실행중이다:
❯ kubectl describe po kube-scheduler-minikube -n kube-system | grep Controlled Controlled By: Node/minikube 레이블 &amp;amp; 셀렉터 레이블: 쿠버네티스 오브젝트를 특정하기 위한 태그.</description></item><item><title>CKA with Practice Tests 정리: Core Concepts</title><link>https://flavono123.github.io/posts/cka-2-core-concept/</link><pubDate>Sat, 15 Jan 2022 17:09:34 +0900</pubDate><guid>https://flavono123.github.io/posts/cka-2-core-concept/</guid><description>쿠버네티스에 대한 관심이 생기며 CKA란 자격증도 알게 됐다. 원랜 개발 관련한 자격증은 막연한 편견이 있었다. 실무와 거리가 멀것만 같은&amp;hellip; 하지만 시험이 프롬프트를 통해 쿠버네티스 상태를 만든다는 점에서 그런 편견이 깨졌다. 무엇보다 회사에서 아직 쿠버네티스를 쓰지 않으니 배울 수 있는 좋은 기회인거 같아 자격증 준비를 시작했다.
아주 유명한 Udemy 강의인 Certified Kubernetes Administrator (CKA) with Practice Tests를 듣고 있다. 강의를 들은 후 섹션별로 내용을 정리할 계획이다.
쿠버네티스 문서화가 잘 되어 있고 한글 번역된것도 많다.</description></item><item><title>Minikube에서 NodePort 서비스 로컬 프라이빗 IP 찾기</title><link>https://flavono123.github.io/posts/minikube-service-private-ip/</link><pubDate>Fri, 07 Jan 2022 10:44:54 +0900</pubDate><guid>https://flavono123.github.io/posts/minikube-service-private-ip/</guid><description>minikube service --url &amp;lt;service-name&amp;gt; NodePort 서비스를 만들어 포트는 노출시켰는데 어느 IP로 접속할지 몰라 한참을 헤맸다&amp;hellip;
출처: https://minikube.sigs.k8s.io/docs/handbook/accessing/#getting-the-nodeport-using-the-service-command</description></item><item><title>Minikube로 Docker Desktop를 대체해보며 쿠버네티스 통빡으로 맞춰보기</title><link>https://flavono123.github.io/posts/minikube-replace-docker-desktop/</link><pubDate>Tue, 04 Jan 2022 00:51:44 +0900</pubDate><guid>https://flavono123.github.io/posts/minikube-replace-docker-desktop/</guid><description>https://novemberde.github.io/post/2021/09/02/podman-minikube/
이 글을 읽고 회사 개발 계정 메일로 오던 도커에서 프로모 코드 뿌리는, 확인 안하던, 메일들이 생각났다.
역시 도커 데스크탑이 유료로 바뀌니 주는 할인 코드였고 마침 쿠버네티스에 대한 관심도가 마구마구 오르는 시기라 호기롭게 위 포스팅을 따라하였다.
도커 데스크탑을 삭제하고 minikube 와 kompose 설치, 기존 docker-compose 파일을 변환해서 apply 까지 마쳤다. 마침 대상으로 해본 docker-compose 의 이미지들은 어차피 로컬(맥)에선 잘 안도는거라, 컨테이너가 잘 띄워졌는지 확인하진 않았다(솔직히 쿠버네티스도 컨테이너를 띄우는가? 도커를 대체한다니깐 그러지 않을까?</description></item></channel></rss>